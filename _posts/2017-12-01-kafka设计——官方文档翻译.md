---
layout:     post
title:      kafka设计——官方文档翻译
date:       2017-12-01
author:     hsd
header-img: img/post-bg-os-metro.jpg
catalog: true
tags:
    - 数据挖掘
    - 机器学习
---
>先mark后写

# 4.设计
## 4.1 动机
&emsp;我们设计kafka使其成为一个能够处理大公司产生的所有实时数据的统一平台，为了做到这点我们不得不考虑相当广泛的使用场景，它需要有高吞吐量来支撑大量的事件流比如实时日志聚合，它得优雅地处理大量的数据backlog（日志？）来支撑周期性地离线系统数据导入，同时意味着系统不得不有低延迟消息交付，来应对更加传统的消息使用场景。
&emsp;We wanted to support partitioned, distributed, real-time processing of these feeds to create new, derived feeds. This motivated our partitioning and consumer model.（中间那个feed不是特别理解）
&emsp;最后在流被送入其他数据系统时，我们知道系统不得不在机器宕机的情况下保证容错
&emsp;支持这些使用场景将我们指引到一种设计，即有一堆唯一的元素（unique elements)，相比传统消息系统而言更像是数据库日志。我们会在接下来的部分概述下设计中的一些元素
## 4.2 持久化
### 不要害怕文件系统
&emsp;Kafka严重地依赖文件系统来存储和缓存消息，人们通常认为硬盘是缓慢的，这使得人们怀疑一个持久化的结构是否能提供一个有竞争力的性能。实际上磁盘比人们想象中要更慢、更快，取决于他们怎么使用，同时一个经过恰当设计的磁盘结构通常能和网络一样快
&emsp;关于磁盘性能很重要的一个事实就是硬盘的吞吐量。结果就是，顺序写一个7200rpm SATA RAID-5 array的JBOD大概是600MB/sec但是随机写只有大约100k/sec，差了超过6000倍...这些顺序读写是所有使用模式中最可预见的，同时被操作系统大量优化过。一个现代的操作系统提供预读和延迟写(write-behind)技术，即读取多个大块数据，将小的逻辑写合并到大的物理写。关于这个话题更深入的讨论可以在[ACM Queue article](http://queue.acm.org/detail.cfm?id=1563874)中找到，他们实际发现[顺序磁盘访问在某些情况下比随机内存访问要快!](http://deliveryimages.acm.org/10.1145/1570000/1563874/jacobs3.jpg)
&emsp;为了弥补这种性能差异，现代操作系统对使用主存做磁盘缓存变得越发激进，一个现代的操作系统会很乐意将所有的空闲内存用来做磁盘缓存，在内存被回收时会有一点性能惩罚。所有的磁盘读写会通过这个统一的cache进行。除非使用直接IO，否则不容易关闭这个特性，因此即使一个进程维护了一个进程内的数据缓存，这些数据很有可能会与OS的页缓存冗余，实际上将所有的都存储了两次。
&emsp;此外，我们构建于JVM之上，在java内存使用上花过点时间的人都知道两件事：
1. 对象的内存开销是很大的，通常是实际数据大小的两倍（或更糟糕）
2. java gc变得越发缓慢和精巧（fiddly）随着堆数据的增加

&emsp;鉴于这些因素，使用文件系统并依赖页缓存优于维护一个内存缓存或者其他结构——我们至少获得了两倍的可用缓存，通过自动访问所有空闲内存（译注：OS页缓存），同时很可能再次加倍通过存储压缩的字节结构而不是独立的对象。这样做使得在一个32GB的机器上cache能达到28-30GB并且没有GC惩罚。进一步，即使服务重启了，这些缓存依然会保持热的状态（stay warm，即热缓存），而进程内缓存需要在内存上重建（10GB的缓存可能会花费10分钟)或者它会以完全冷的缓存启动（这通常意味着糟糕的初始性能）。这也极大简化了代码，因为所有的用来保持内存和文件系统一致性的逻辑都交给了OS，相比一次性的进程内尝试而言，OS通常能更高效更准确的完成这些事情。如果你的磁盘访问模式偏好于顺序读那么预读技术在每次磁盘访问时都会有效地获取有用的数据
这使得好的设计变得非常简单：不是以维护尽可能多的内存数据然后在我们内存空间不足时将其刷新到文件系统的方式，我们反过来做。所有的数据都立即被写到文件系统中的持久化日志中，实际上这仅仅意味这它被传输到内核的页缓存中。
这种以页缓存为中心的设计风格在这篇关于Varnish的设计的[文章](http://varnish-cache.org/wiki/ArchitectNotes)中有描述（带着一种健康的自大）
### Constant Time Suffices
&emsp;消息系统中使用的持久化数据结构通常是每个消费者一个队列同时有一个用于保存消息元数据的相关的B树或者其他通用目的的随机访问的数据结构。B树是用处最通用的数据结构，能支持消息系统中各种事务和非事务的语义。磁盘访问一次10ms，每次只能做一个搜寻因此并发性收到了限制。因此虽然只是少量的磁盘访问也会有非常高的开销。因为存储系统混合了非常快的缓存操作和非常缓慢的物理磁盘操作，观察到的树形结构的性能通常随着固定缓存的数据的增加呈超线性关系的——即加倍你的数据会使得访问比两倍还慢
&emsp;直觉来说在简单读和追加到文件的情况下可以使用一个持久化的队列，这通常是logging的解决方法，这个结构有一个优点那就是所有的操作都是O(1)的并且读不会阻塞写反之亦然。这有个明显的性能优势因为性能完全和数据大小解耦开来——一个服务器能充分利用许多廉价的低转速的1+TB的SATA驱动器，尽管它们的寻址性能比比较差，对于大规模读写来说这些驱动器仍有着可以接受的性能同时有着1/3的价格和3倍的容量
&emsp;能够访问几乎是不受限的磁盘空间并且没有任何性能惩罚意味着我们能够提供一些消息系统中不常见的特性。比如说，在kafka里，我们在一个相对较长的时间段(比如说一周)内保留消息，而不是它们一被消费我们就删了它们，这给消费者带来了很多灵活性，我们之后将会进行描述。
## 4.3 效率
&emsp;我们在效率上做了很大的努力，一个我们主要的用例场景就是处理web活动数据，数据量往往很大：每一次网页访问会产生数以十计的写。而且，我们假设一个产生的消息至少会被一个消费者（通常很多）读取，因此我们努力使消息消费变得尽可能的廉价
从搭建和运行许多类似的系统的经验中我们同时发现，效率是高效的多租户操作的关键。如果下流的基础组件服务因应用使用模式的微小变化就轻易地变成瓶颈的话，这样的微小变动通常会带来问题。通过变得很快我们

。



