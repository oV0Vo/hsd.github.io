---
layout:     post
title:      kafka设计——官方文档翻译
date:       2017-12-06
author:     hsd
header-img: img/post-bg-os-metro.jpg
catalog: true
tags:
    - 消息中间件
    - 后台框架
---
>kafka官方文档设计部分的翻译，持续更新中

# 4.设计
## 4.1 动机

我们设计kafka使其成为一个能够处理大公司产生的所有实时数据的统一平台，为了做到这点我们不得不考虑相当广泛的使用场景，它需要有高吞吐量来支撑大量的事件流比如实时日志聚合，它得优雅地处理大量的数据backlog（日志？）来支撑周期性地离线系统数据导入，同时意味着系统不得不有低延迟消息交付，来应对更加传统的消息使用场景.

We wanted to support partitioned, distributed, real-time processing of these feeds to create new, derived feeds. This motivated our partitioning and consumer model.（中间那个feed不是特别理解）

最后在流被送入其他数据系统时，我们知道系统不得不在机器宕机的情况下保证容错

支持这些使用场景将我们指引到一种设计，即有一堆唯一的元素（unique elements)，相比传统消息系统而言更像是数据库日志。我们会在接下来的部分概述下设计中的一些元素
## 4.2 持久化
### 不要害怕文件系统
Kafka严重地依赖文件系统来存储和缓存消息，人们通常认为硬盘是缓慢的，这使得人们怀疑一个持久化的结构是否能提供一个有竞争力的性能。实际上磁盘比人们想象中要更慢、更快，取决于他们怎么使用，同时一个经过恰当设计的磁盘结构通常能和网络一样快

关于磁盘性能很重要的一个事实就是硬盘的吞吐量。结果就是，顺序写一个7200rpm SATA RAID-5 array的JBOD大概是600MB/sec但是随机写只有大约100k/sec，差了超过6000倍...这些顺序读写是所有使用模式中最可预见的，同时被操作系统大量优化过。一个现代的操作系统提供预读和延迟写(write-behind)技术，即读取多个大块数据，将小的逻辑写合并到大的物理写。关于这个话题更深入的讨论可以在[ACM Queue article](http://queue.acm.org/detail.cfm?id=1563874)中找到，他们实际发现[顺序磁盘访问在某些情况下比随机内存访问要快!](http://deliveryimages.acm.org/10.1145/1570000/1563874/jacobs3.jpg)

为了弥补这种性能差异，现代操作系统对使用主存做磁盘缓存变得越发激进，一个现代的操作系统会很乐意将所有的空闲内存用来做磁盘缓存，在内存被回收时会有一点性能惩罚。所有的磁盘读写会通过这个统一的cache进行。除非使用直接IO，否则不容易关闭这个特性，因此即使一个进程维护了一个进程内的数据缓存，这些数据很有可能会与OS的页缓存冗余，实际上将所有的都存储了两次。

此外，我们构建于JVM之上，在java内存使用上花过点时间的人都知道两件事：
1. 对象的内存开销是很大的，通常是实际数据大小的两倍（或更糟糕）
2. java gc变得越发缓慢和精巧（fiddly）随着堆数据的增加

鉴于这些因素，使用文件系统并依赖页缓存优于维护一个内存缓存或者其他结构——我们至少获得了两倍的可用缓存，通过自动访问所有空闲内存（译注：OS页缓存），同时很可能再次加倍通过存储压缩的字节结构而不是独立的对象。这样做使得在一个32GB的机器上cache能达到28-30GB并且没有GC惩罚。进一步，即使服务重启了，这些缓存依然会保持热的状态（stay warm，即热缓存），而进程内缓存需要在内存上重建（10GB的缓存可能会花费10分钟)或者它会以完全冷的缓存启动（这通常意味着糟糕的初始性能）。这也极大简化了代码，因为所有的用来保持内存和文件系统一致性的逻辑都交给了OS，相比一次性的进程内尝试而言，OS通常能更高效更准确的完成这些事情。如果你的磁盘访问模式偏好于顺序读那么预读技术在每次磁盘访问时都会有效地获取有用的数据

这使得好的设计变得非常简单：不是以维护尽可能多的内存数据然后在我们内存空间不足时将其刷新到文件系统的方式，我们反过来做。所有的数据都立即被写到文件系统中的持久化日志中，实际上这仅仅意味这它被传输到内核的页缓存中。

这种以页缓存为中心的设计风格在这篇关于Varnish的设计的[文章](http://varnish-cache.org/wiki/ArchitectNotes)中有描述（带着一种健康的自大）
### Constant Time Suffices
消息系统中使用的持久化数据结构通常是每个消费者一个队列同时有一个用于保存消息元数据的相关的B树或者其他通用目的的随机访问的数据结构。B树是用处最通用的数据结构，能支持消息系统中各种事务和非事务的语义。磁盘访问一次10ms，每次只能做一个搜寻因此并发性收到了限制。因此虽然只是少量的磁盘访问也会有非常高的开销。因为存储系统混合了非常快的缓存操作和非常缓慢的物理磁盘操作，观察到的树形结构的性能通常随着固定缓存的数据的增加呈超线性关系的——即加倍你的数据会使得访问比两倍还慢

直觉来说在简单读和追加到文件的情况下可以使用一个持久化的队列，这通常是logging的解决方法，这个结构有一个优点那就是所有的操作都是O(1)的并且读不会阻塞写反之亦然。这有个明显的性能优势因为性能完全和数据大小解耦开来——一个服务器能充分利用许多廉价的低转速的1+TB的SATA驱动器，尽管它们的寻址性能比比较差，对于大规模读写来说这些驱动器仍有着可以接受的性能同时有着1/3的价格和3倍的容量

能够访问几乎是不受限的磁盘空间并且没有任何性能惩罚意味着我们能够提供一些消息系统中不常见的特性。比如说，在kafka里，我们在一个相对较长的时间段(比如说一周)内保留消息，而不是它们一被消费我们就删了它们，这给消费者带来了很多灵活性，我们之后将会进行描述。
## 4.3 效率
我们在效率上做了很大的努力，一个我们主要的用例场景就是处理web活动数据，数据量往往很大：每一次网页访问会产生数以十计的写。而且，我们假设一个产生的消息至少会被一个消费者（通常很多）读取，因此我们努力使消息消费变得尽可能的廉价

从搭建和运行许多类似的系统的经验中我们同时发现，效率是高效的多租户操作的关键。如果下流的基础组件服务因应用使用模式的微小变化就轻易地变成瓶颈的话，这样的微小变动通常会带来问题。By being very fast we help ensure that the application will tip-over under load before the infrastructure。对于尝试在中心化集群运行一个支持数十或数百个应用的中心化服务时这显得尤为重要，因为使用模式的改变近乎是每天都会发生的事

我们在前面的章节中讨论了磁盘效率，当消除了不良的磁盘访问模式后，还会造成系统低效的两个通常原因位：太多小io操作，以及过多的字节复制

小io问题通常发生在客户端和服务器之间（译注：网络传输）以及服务器自身的持久化操作

为了避免这个问题，我们的协议围绕着message set的抽象构造，message set将消息自然地组合在一起。这允许网络请求将消息整合再一起，均摊了网络往返的开销，而不是每一次发送一个单一的消息。服务器一次将消息块追加到log中，然后消费者每次读取大的线性块

这个简单的优化带来了数量级的加速，批量使得网络包更大，更大的线性磁盘操作，连续的内存块，等等，所有的这些让kafka可以将猝发的随机消息流转变成流向消费者的线性写

另一个造成低效的是字节复制，在低消息速率的情况下这不是个问题，但在运行负载下影响是很显著的。为了避免这个问题我们使用了一个可以在producer、broker、consumer之间共享的标准的二进制的消息格式（因此数据块可以不用被修改就在它们之间传输）

broker维护的消息日志只是一个文件目录，每一个文件由message set序列填充，message set在producer和consumer之间保留着相同的格式。维护这个通用的格式允许对最重要的操作的优化：对持久化日志块的网络传输。现代的unix操作系统提供了一个高度优化过的代码路径来将数据从pagecache中传输到socket上；在linux上这由sendfile系统调用完成

为了理解sendfile的影响，理解将数据从文件到socket的传输的通用代码路径是很重要的：
1. 操作系统将数据从磁盘读入到内核空间中的pagecache中
2. 应用将数据从内核空间读入到用户空间缓存中
3. 应用将数据写回内核空间中的socket缓存中
4. 操作系统将socket缓存中的数据复制到nic缓存中，nic缓存中的数据将会被发送到网络上

这显然是低效的，需要经过四次copy和两次系统调用。通过使用sendfile，这些re-copying可以被消除，通过让OS直接将数据从页缓存（译注：内核空间中）发送到网络上。因此在这个优化后的路径，只需要最后的复制到nic缓存上

我们预料的一个通用的用例场景就是一个topic上有多个consumer，通过使用上面的zero-copy优化，数据正好复制到页缓存一次然后每次消费都能重用，而不是存储在内存中然后每次读取都复制到用户空间中。这允许消息能以接近网络连接限制的速率消费

pagecache和sendfile的结合意味着在一个大多数消费者都追上的kafka集群上你看不到有读磁盘的活动，因为它们都直接从缓存中提供数据

关于sendfile和java对zero-copy的支持的更多背景知识可以查看这篇[文章](http://www.ibm.com/developerworks/linux/library/j-zerocopy)

## 端到端批量压缩
在一些情况下瓶颈实际上不是CPU也不是磁盘而是网络带宽，对于需要通过广域网在数据中心间发送消息的数据流水线来说这尤为正确。当然，没有来自kafka的支持，用户也总是可以每个消息都进行压缩，但这回导致非常差的压缩率，因为大多数的冗余都是因为同类型消息间的重复（比如json中的字段名称或者web日志中的user&ensp;agents或者相同的字符串）。高效的压缩要求同时压缩多个消息而不是对每个消息都单独进行压缩

kafka通过一个高效的批量格式来支持这个，一批消息可以一起被压缩然后以这种形式发送到服务器上，这批消息会以压缩的格式写入，在log中保持压缩的状态然后只会被消费者解压

Kafka支持gzip、snappy、lz4压缩协议，关于压缩更多的细节可以查看[这里](https://cwiki.apache.org/confluence/display/KAFKA/Compression)

# 4.4 生产者
## 负载均衡
生产者直接将数据发送到partition leader所在的broker上，没有任何中间的路由层，为了帮助生产者做到这个所有的kafka节点都可以回答关于那个服务器活着还有任意时刻一个topic的partions的leaders都在哪些节点上的元数据请求，这让生产者可以恰当地发送它的请求

用户控制消息将会发到哪个分区上，可以是随机的，实现一种随机的负载均衡，也可以由一些有语义的分区函数。我们将有语义的分区接口暴露给用户，用户可以通过指定一个分区的key然后使用这个key来hash到一个分区上（如果需要的话也可以覆盖分区函数）。例如，如果选择的分区的key为user id，那么一个特定用户所有的数据都会被发送到相同的分区上。这反过来允许消费者假定它们的消费都是本地的（make locality assumptions about therir consumption），这种精心设计的分区风格允许本地的敏感的消费者处理

## 异步发送

批量是效率的一大驱动，为了使批量成为可能kafka生产者会尝试在内存中累计数据，然后用一个单一的请求将更大的批次发送出去。batching可以被配置为累积不超过一个固定数量的消息，等待不超过一个固定的时延（比如64k或10ms）。这允许累计更多要发送的字节，以及server上少的更大的io操作，这个缓存是可以被配置的，提供了一个机制来权衡小数量的额外延迟来获得更高的吞吐量

[配置细节](http://kafka.apache.org/documentation.html#producerconfigs)和[生产者api](http://kafka.apache.org/082/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html)可以在此文档的别处找到

# 4.5 消费者

kafka消费者通过发送获取请求到它想消费的分区所在的leader broker上工作，对于每个请求，消费者指定其在log中的offset，然后收到从此位置开始的一整块的日志。消费者因此对消费的位置能有很好的控制，并且如果需要的话可以重置位置来重新消费数据

## push vs pull

我们一开始考虑的一个问题是消费者应该是从brokers上pull数据呢还是broker将数据push到消费者上，在这方面kafka遵循了一个被大多数消息系统所共享，更为传统的设计，即将数据是从生产者push到broker上然后consumer从broker上pull数据。一些以日志为中心的系统，比如[Scribe](http://github.com/facebook/scribe)和[Apache Flume](http://flume.apache.org/)，使用了一个非常不同的基于push的方式，数据是被push到下流的。两种方式都各有其优缺点，然而，基于push的系统难以处理不同的消费者（译注：消费能力不同）因为数据传输速率是由broker控制的。目标通常是让消费者能以最大可能的速率进行消费，不幸的是，在一个push的系统中这意味着当消费的消费速率低于生产的速率时（大体上为DOS攻击），消费者倾向于过载。一个基于pull的系统有着更好的特点，即消费者只是简单地落后了然后当它能够追上时追上。通过一些消费者可以暗示它过载的回退协议，问题可以得到缓和，然而让传输速率能充分利用（但永远不要过度利用）消费者比它看上去的到更棘手。之前的一些使用这种风格构建系统的尝试使得我们投入了更传统的pull模型

基于pull的系统的另外一个有点就是它适用于激进地批量化发送给消费者的数据，一个基于push的系统必须选择是立即发送请求还是累积更多的数据然后稍后在不知道下游消费者是否能立即处理消息的情况下发送。如果调试为低延迟，这回使得为了让传输被缓冲然后每次都发送一个消息，这是浪费的。一个基于pull的设计解决了这个问题，因为消费者总是拉取在它在log中位置以后的所有可用消息（或者到某些配置的最大个数）。因为我们在没有引入不必要的延迟的情况下可以得到优化的batching

简单的基于pull的系统的缺点就是如果broker没有数据那么消费者可能会不停地循环pull，高效地忙等待数据的到达。为了避免这个在我们的pull请求中有一些参数可以让消费者请求阻塞在一个long poll等待知道数据到达（也可选等待直到有给定数据的字节来确保大的传输大小）

你可以想象在其他可能的只有pull的端到端的设计，消费者会本地写到一个本地的日志中，然后broker会pull然后消费者会pull broker，通常会有一个“store-and-forward”的类似类型的生产者。这是有趣的但我们感觉不适用于我们的目标用例场景，这些场景有着成千的生产者。我们跑大规模的持久化数据系统的经验让我们感觉到跨越多个应用涉及到系统中成千的磁盘实际上并不会让事情变得更可靠，最后会变成运维的梦魇。在实践中我们同时发现在不需要生产者持久化的情况下依旧可以跑一个大规模并有着强SLA的流水线。

## 消费者位置

惊奇的是，记录哪些已经被消费了是消息系统中一个关键的性能点

大多数消息系统在broker上保留哪些消息已经被消费了的元数据，即，当消息传给消费者时，broker要不立即本地记录或等待消费者的ack。这是一个相当直觉的选择，且对于一个单一的服务器来说确实不清楚这个状态可以保存到其他哪里。因为许多消息系统使用的数据存储结构都是很难扩展的，这同时也是一个实用的选择——因为broker知道哪些已经被消费了，这样它可以立即删除它，保持小的数据大小

没那么显而易见的是让broker和消费者对哪些已经被消费了达成共识并不是一个简单的问题。如果broker在每次消息通过网络传出去的时候都记录消息是已经被消费了的话，如果消费者处理消息失败（比如因为它宕机了或者请求超时了或其他任何情况），那么消息就丢失了。为了解决这个问题，许多消息系统增加了ack的特性，这意味这消息只是被标记为sent而不是consumed当消息被发送时。这个策略解决了消息的丢失问题，但带了了新的问题。首先，如果消费者消费了消息但发送ack失败，那么消息会被消费两次。第二个问题是关于性能，现在对于每一个单一的消息broker必须维护多个状态（首先对它加锁这样它就不会发送两次， 然后标记它为永久被消费了的因此可以删除它），必须解决的棘手问题，比如怎么处理发送了但是从没被ack的消息。

Kafka处理的方式不同，我们的topic被分为完全有序的partition的集合，在任意是个每一partition都被每个订阅的消费者组中的恰好一个消费者消费。这意味着每个分区上消费者的位置只是一个整数，表示下一个要消费的消息的offset。这使得关于哪些已经被消费了的的状态很小，对于每个partition来说只是一个数字，这个状态可以被周期性地checkpoint，这使得响应的消息ack开销很小。

这个决定有个副作用，一个消费者可以故意地重置到一个老的offset然后重新消费数据。这破坏了队列的通用合同，但对许多消费者来说被证明是一个本质的特征。例如，如果消费者的代码出bug了，并且是在消费了许多消息后才发现的，那么消费者可以在修复bug后重新消费这些消息。

## 离线数据负载

可扩展的持久化让周期性的消费，比如周期性地批量装载数据到形如hadoop或关系型数据库的离线系统，这样的批量数据负载变得可能

In the case of Hadoop we parallelize the data load by splitting the load over individual map tasks, one for each node/topic/partition combination, allowing full parallelism in the loading. Hadoop provides the task management, and tasks which fail can restart without danger of duplicate data—they simply restart from their original position.

# 4.6 消息传递语义
既然我们对的生产者和消费者如何工作有了一点了解，让我们来讨论kafka提供的生产者和消费者之间的语义保证。清晰的是有着多种可能可以提供的消息传递语义
- At most once——消息可能丢失但永远不会重复投递
- At least once——消息永远不会丢失但可能会重复投递
- exactly once——这是人们想要的，消息不丢失且只会消费一次

值得注意的是这分解成两个问题：消息publish的持久性保障和消息消费的保障
许多系统声称提供exactly once的传递语义，但仔细阅读是重要的，许多这些声称都是有误导性的（即，它们不考虑消费者或生产者失败的情况，有着多个消费者进程的情况，写到磁盘中的数据可能会丢失的情况）

kafka的语义是很直接的，当public一个消息时我们有一个概念叫消息被commit到log里，一旦一个publish的消息被提交后，只要有一个复制了消息写到的分区的broker存活的话，消息就不会丢失。committed消息的定义，存活的分区以及关于我们尝试解决哪种类型的失败的描述将会在下一个章节进行详细描述。对与现在让我们假设有一个完美的不会丢失的broker，尝试来理解对生产者和消费者的保证。如果一个生长着尝试发布一个消息然后经历了一次网络错误，它不能确认错误是发生在消息committed之前还是之后。这类似于使用自动生成（auto-generated）的key插入到一个数据库表的语义
0.11.0.0之前，如果一个生产者接收消息commited的应答失败，除了重新发送消息外没有其他好的选择。这提供了at-least-once的传递语义，因为如果原来的请求成功了的话消息会被再次写入log。从0.11.0.0开始，kafka生产者也支持幂等的传递选项，保证了重新发送不会导致log中出现重复的项。为了实现这个，broker给每一个生产者指定一个ID然后